{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide ResNet 示例 - TensorLayer实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from tensorlayer.layers import set_keep\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNNEnv:\n",
    "    def __init__(self):\n",
    "\n",
    "        # The data, shuffled and split between train and test sets\n",
    "        self.x_train, self.y_train, self.x_test, self.y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3), plotable=False)\n",
    "\n",
    "        # Reorder dimensions for tensorflow\n",
    "        self.mean = np.mean(self.x_train, axis=0, keepdims=True)\n",
    "        self.std = np.std(self.x_train)\n",
    "        self.x_train = (self.x_train - self.mean) / self.std\n",
    "        self.x_test = (self.x_test - self.mean) / self.std\n",
    "\n",
    "        print('x_train shape:', self.x_train.shape)\n",
    "        print('x_test shape:', self.x_test.shape)\n",
    "        print('y_train shape:', self.y_train.shape)\n",
    "        print('y_test shape:', self.y_test.shape)\n",
    "\n",
    "        # For generator\n",
    "        self.num_examples = self.x_train.shape[0]\n",
    "        self.index_in_epoch = 0\n",
    "        self.epochs_completed = 0\n",
    "\n",
    "        # For wide resnets\n",
    "        self.blocks_per_group = 4\n",
    "        self.widening_factor = 4\n",
    "\n",
    "        # Basic info\n",
    "        self.batch_num = 64\n",
    "        self.img_row = 32\n",
    "        self.img_col = 32\n",
    "        self.img_channels = 3\n",
    "        self.nb_classes = 10\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        start = self.index_in_epoch\n",
    "        self.index_in_epoch += self.batch_size\n",
    "\n",
    "        if self.index_in_epoch > self.num_examples:\n",
    "            # Finished epoch\n",
    "            self.epochs_completed += 1\n",
    "            # Shuffle the data\n",
    "            perm = np.arange(self.num_examples)\n",
    "            np.random.shuffle(perm)\n",
    "            self.x_train = self.x_train[perm]\n",
    "            self.y_train = self.y_train[perm]\n",
    "\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self.index_in_epoch = self.batch_size\n",
    "            assert self.batch_size <= self.num_examples\n",
    "        end = self.index_in_epoch\n",
    "        return self.x_train[start:end], self.y_train[start:end]\n",
    "\n",
    "    def reset(self, first):\n",
    "        self.first = first\n",
    "        if self.first is True:\n",
    "            self.sess.close()\n",
    "\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "    def step(self):\n",
    "\n",
    "        def zero_pad_channels(x, pad=0):\n",
    "            \"\"\"\n",
    "            Function for Lambda layer\n",
    "            \"\"\"\n",
    "            pattern = [[0, 0], [0, 0], [0, 0], [pad - pad // 2, pad // 2]]\n",
    "            return tf.pad(x, pattern)\n",
    "\n",
    "        def residual_block(x, count, nb_filters=16, subsample_factor=1):\n",
    "            prev_nb_channels = x.outputs.get_shape().as_list()[3]\n",
    "\n",
    "            if subsample_factor > 1:\n",
    "                subsample = [1, subsample_factor, subsample_factor, 1]\n",
    "                # shortcut: subsample + zero-pad channel dim\n",
    "                name_pool = 'pool_layer' + str(count)\n",
    "                shortcut = tl.layers.PoolLayer(x,\n",
    "                                               ksize=subsample,\n",
    "                                               strides=subsample,\n",
    "                                               padding='VALID',\n",
    "                                               pool=tf.nn.avg_pool,\n",
    "                                               name=name_pool)\n",
    "\n",
    "            else:\n",
    "                subsample = [1, 1, 1, 1]\n",
    "                # shortcut: identity\n",
    "                shortcut = x\n",
    "\n",
    "            if nb_filters > prev_nb_channels:\n",
    "                name_lambda = 'lambda_layer' + str(count)\n",
    "                shortcut = tl.layers.LambdaLayer(\n",
    "                    shortcut,\n",
    "                    zero_pad_channels,\n",
    "                    fn_args={'pad': nb_filters - prev_nb_channels},\n",
    "                    name=name_lambda)\n",
    "\n",
    "            name_norm = 'norm' + str(count)\n",
    "            y = tl.layers.BatchNormLayer(x,\n",
    "                                         decay=0.999,\n",
    "                                         epsilon=1e-05,\n",
    "                                         is_train=True,\n",
    "                                         name=name_norm)\n",
    "\n",
    "            name_conv = 'conv_layer' + str(count)\n",
    "            y = tl.layers.Conv2dLayer(y,\n",
    "                                      act=tf.nn.relu,\n",
    "                                      shape=[3, 3, prev_nb_channels, nb_filters],\n",
    "                                      strides=subsample,\n",
    "                                      padding='SAME',\n",
    "                                      name=name_conv)\n",
    "\n",
    "            name_norm_2 = 'norm_second' + str(count)\n",
    "            y = tl.layers.BatchNormLayer(y,\n",
    "                                         decay=0.999,\n",
    "                                         epsilon=1e-05,\n",
    "                                         is_train=True,\n",
    "                                         name=name_norm_2)\n",
    "\n",
    "            prev_input_channels = y.outputs.get_shape().as_list()[3]\n",
    "            name_conv_2 = 'conv_layer_second' + str(count)\n",
    "            y = tl.layers.Conv2dLayer(y,\n",
    "                                      act=tf.nn.relu,\n",
    "                                      shape=[3, 3, prev_input_channels, nb_filters],\n",
    "                                      strides=[1, 1, 1, 1],\n",
    "                                      padding='SAME',\n",
    "                                      name=name_conv_2)\n",
    "\n",
    "            name_merge = 'merge' + str(count)\n",
    "            out = tl.layers.ElementwiseLayer([y, shortcut],\n",
    "                                             combine_fn=tf.add,\n",
    "                                             name=name_merge)\n",
    "\n",
    "\n",
    "            return out\n",
    "\n",
    "        # Placeholders\n",
    "        learning_rate = tf.placeholder(tf.float32)\n",
    "        img = tf.placeholder(tf.float32, shape=[self.batch_num, 32, 32, 3])\n",
    "        labels = tf.placeholder(tf.int32, shape=[self.batch_num, ])\n",
    "\n",
    "        x = tl.layers.InputLayer(img, name='input_layer')\n",
    "        x = tl.layers.Conv2dLayer(x,\n",
    "                                  act=tf.nn.relu,\n",
    "                                  shape=[3, 3, 3, 16],\n",
    "                                  strides=[1, 1, 1, 1],\n",
    "                                  padding='SAME',\n",
    "                                  name='cnn_layer_first')\n",
    "\n",
    "        for i in range(0, self.blocks_per_group):\n",
    "            nb_filters = 16 * self.widening_factor\n",
    "            count = i\n",
    "            x = residual_block(x, count, nb_filters=nb_filters, subsample_factor=1)\n",
    "\n",
    "        for i in range(0, self.blocks_per_group):\n",
    "            nb_filters = 32 * self.widening_factor\n",
    "            if i == 0:\n",
    "                subsample_factor = 2\n",
    "            else:\n",
    "                subsample_factor = 1\n",
    "            count = i + self.blocks_per_group\n",
    "            x = residual_block(x, count, nb_filters=nb_filters, subsample_factor=subsample_factor)\n",
    "\n",
    "        for i in range(0, self.blocks_per_group):\n",
    "            nb_filters = 64 * self.widening_factor\n",
    "            if i == 0:\n",
    "                subsample_factor = 2\n",
    "            else:\n",
    "                subsample_factor = 1\n",
    "            count = i + 2*self.blocks_per_group\n",
    "            x = residual_block(x, count, nb_filters=nb_filters, subsample_factor=subsample_factor)\n",
    "\n",
    "        x = tl.layers.BatchNormLayer(x,\n",
    "                                     decay=0.999,\n",
    "                                     epsilon=1e-05,\n",
    "                                     is_train=True,\n",
    "                                     name='norm_last')\n",
    "\n",
    "        x = tl.layers.PoolLayer(x,\n",
    "                                ksize=[1, 8, 8, 1],\n",
    "                                strides=[1, 8, 8, 1],\n",
    "                                padding='VALID',\n",
    "                                pool=tf.nn.avg_pool,\n",
    "                                name='pool_last')\n",
    "\n",
    "        x = tl.layers.FlattenLayer(x, name='flatten')\n",
    "\n",
    "        x = tl.layers.DenseLayer(x,\n",
    "                                 n_units=self.nb_classes,\n",
    "                                 act=tf.identity,\n",
    "                                 name='fc')\n",
    "\n",
    "        output = x.outputs\n",
    "\n",
    "        ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=labels))\n",
    "        cost = ce\n",
    "\n",
    "        correct_prediction = tf.equal(tf.cast(tf.argmax(output, 1), tf.int32), labels)\n",
    "        acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        train_params = x.all_params\n",
    "        train_op = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate, use_locking=False).minimize(cost, var_list=train_params)\n",
    "\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        for i in range(10):\n",
    "            batch = self.next_batch(self.batch_num)\n",
    "            feed_dict = {img: batch[0], labels: batch[1], learning_rate: 0.01}\n",
    "            feed_dict.update(x.all_drop)\n",
    "            _, l, ac = self.sess.run([train_op, cost, acc], feed_dict=feed_dict)\n",
    "            print('loss', l)\n",
    "            print('acc', ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load or Download cifar10 > data/cifar10/\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "x_test shape: (10000, 32, 32, 3)\n",
      "y_train shape: (50000,)\n",
      "y_test shape: (10000,)\n",
      "  [TL] InputLayer  input_layer: (64, 32, 32, 3)\n",
      "  [TL] Conv2dLayer cnn_layer_first: shape:[3, 3, 3, 16] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] LambdaLayer  lambda_layer0\n",
      "  [TL] BatchNormLayer norm0: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer0: shape:[3, 3, 16, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] BatchNormLayer norm_second0: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer_second0: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] ElementwiseLayer merge0: size:(64, 32, 32, 64) fn:add\n",
      "  [TL] BatchNormLayer norm1: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer1: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] BatchNormLayer norm_second1: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer_second1: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] ElementwiseLayer merge1: size:(64, 32, 32, 64) fn:add\n",
      "  [TL] BatchNormLayer norm2: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer2: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] BatchNormLayer norm_second2: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer_second2: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] ElementwiseLayer merge2: size:(64, 32, 32, 64) fn:add\n",
      "  [TL] BatchNormLayer norm3: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer3: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] BatchNormLayer norm_second3: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer_second3: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] ElementwiseLayer merge3: size:(64, 32, 32, 64) fn:add\n",
      "  [TL] PoolLayer   pool_layer4: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:VALID pool:avg_pool\n",
      "  [TL] LambdaLayer  lambda_layer4\n",
      "  [TL] BatchNormLayer norm4: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer4: shape:[3, 3, 64, 128] strides:[1, 2, 2, 1] pad:SAME act:relu\n",
      "  [TL] BatchNormLayer norm_second4: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer_second4: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] ElementwiseLayer merge4: size:(64, 16, 16, 128) fn:add\n",
      "  [TL] BatchNormLayer norm5: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer5: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] BatchNormLayer norm_second5: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer_second5: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] ElementwiseLayer merge5: size:(64, 16, 16, 128) fn:add\n",
      "  [TL] BatchNormLayer norm6: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer6: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] BatchNormLayer norm_second6: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer_second6: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] ElementwiseLayer merge6: size:(64, 16, 16, 128) fn:add\n",
      "  [TL] BatchNormLayer norm7: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer7: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] BatchNormLayer norm_second7: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer_second7: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] ElementwiseLayer merge7: size:(64, 16, 16, 128) fn:add\n",
      "  [TL] PoolLayer   pool_layer8: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:VALID pool:avg_pool\n",
      "  [TL] LambdaLayer  lambda_layer8\n",
      "  [TL] BatchNormLayer norm8: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer8: shape:[3, 3, 128, 256] strides:[1, 2, 2, 1] pad:SAME act:relu\n",
      "  [TL] BatchNormLayer norm_second8: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer_second8: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] ElementwiseLayer merge8: size:(64, 8, 8, 256) fn:add\n",
      "  [TL] BatchNormLayer norm9: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer9: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] BatchNormLayer norm_second9: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer_second9: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] ElementwiseLayer merge9: size:(64, 8, 8, 256) fn:add\n",
      "  [TL] BatchNormLayer norm10: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer10: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] BatchNormLayer norm_second10: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer_second10: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] ElementwiseLayer merge10: size:(64, 8, 8, 256) fn:add\n",
      "  [TL] BatchNormLayer norm11: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer11: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] BatchNormLayer norm_second11: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer conv_layer_second11: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] ElementwiseLayer merge11: size:(64, 8, 8, 256) fn:add\n",
      "  [TL] BatchNormLayer norm_last: decay:0.999000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] PoolLayer   pool_last: ksize:[1, 8, 8, 1] strides:[1, 8, 8, 1] padding:VALID pool:avg_pool\n",
      "  [TL] FlattenLayer flatten: 256\n",
      "  [TL] DenseLayer  fc: 10 identity\n",
      "WARNING:tensorflow:From /home/xrong/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "loss 2.36917\n",
      "acc 0.109375\n",
      "loss 2.4051\n",
      "acc 0.09375\n",
      "loss 2.2008\n",
      "acc 0.140625\n",
      "loss 2.2007\n",
      "acc 0.140625\n",
      "loss 2.16373\n",
      "acc 0.1875\n",
      "loss 2.11241\n",
      "acc 0.15625\n",
      "loss 2.03776\n",
      "acc 0.21875\n",
      "loss 2.09379\n",
      "acc 0.21875\n",
      "loss 2.13162\n",
      "acc 0.1875\n",
      "loss 2.00087\n",
      "acc 0.265625\n"
     ]
    }
   ],
   "source": [
    "a = CNNEnv()\n",
    "a.reset(first=False)\n",
    "a.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
